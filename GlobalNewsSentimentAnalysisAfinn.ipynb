{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in ./opt/anaconda3/lib/python3.7/site-packages (0.0.45)\n",
      "Requirement already satisfied: textsearch in ./opt/anaconda3/lib/python3.7/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: pyahocorasick in ./opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in ./opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/Users/victornathanael/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: afinn in ./opt/anaconda3/lib/python3.7/site-packages (0.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/Users/victornathanael/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Installing Afinn Lexicon\n",
    "!pip install contractions\n",
    "!pip install afinn\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries for Processing the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import re,string,unicodedata\n",
    "%matplotlib inline\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries For Text Preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/victornathanael/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/victornathanael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victornathanael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing the Dict\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text name  \\\n",
      "0           1  Theresa May has struck a deal with Brussels th...   UK   \n",
      "1           2  The agreement may give UK firms continued acce...   UK   \n",
      "2           3  Nobody knows when, but the Brexit secretary, D...   UK   \n",
      "3           4  There's no shame in asking voters to express t...   UK   \n",
      "4           5  Dominic Raab warns that \"obstacles remain\" but...   UK   \n",
      "\n",
      "         Date  sentiment  \n",
      "0  2018-11-01          0  \n",
      "1  2018-11-01          1  \n",
      "2  2018-11-01          0  \n",
      "3  2018-11-01          0  \n",
      "4  2018-11-01          0  \n"
     ]
    }
   ],
   "source": [
    "#Import the dataset\n",
    "data = pd.read_csv(\"data.csv\", encoding= 'unicode_escape')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text\n",
      "0           1  Theresa May has struck a deal with Brussels th...\n",
      "1           2  The agreement may give UK firms continued acce...\n",
      "2           3  Nobody knows when, but the Brexit secretary, D...\n",
      "3           4  There's no shame in asking voters to express t...\n",
      "4           5  Dominic Raab warns that \"obstacles remain\" but...\n"
     ]
    }
   ],
   "source": [
    "#Remove unused column (Name and Date) and missing values\n",
    "data = data.drop(columns = ['name','Date','sentiment'])\n",
    "data = data.dropna()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text\n",
      "0           1  theresa may struck deal brussels would give uk...\n",
      "1           2  agreement may give uk firms continued access e...\n",
      "2           3  nobody knows brexit secretary dominic raab lik...\n",
      "3           4  shame asking voters express views second time ...\n",
      "4           5  dominic raab warns obstacles remain insists en...\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#Case Folding\n",
    "data['text'] = data['text'].str.lower() #Lowercase the sentence\n",
    "\n",
    "#Tokenizing \n",
    "data['text'] = data['text'].str.strip() #Remove Leading Space and Trailing Space\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "  for punctuations in punctuation:\n",
    "    text = text.replace(punctuations, '')\n",
    "  return text\n",
    "\n",
    "data['text'] = data['text'].apply(remove_punct) #Remove Punctuation\n",
    "\n",
    "def remove_special_char(text, remove_digits=True):\n",
    "  pattern = r'[^a-zA-z0-9\\s]'\n",
    "  text = re.sub(pattern, '', text)\n",
    "  return text\n",
    "\n",
    "data['text'] = data['text'].apply(remove_special_char) #Remove Symbols or other special characters\n",
    "\n",
    "def expand_contractions(con_text):\n",
    "  con_text = contractions.fix(con_text)\n",
    "  return con_text\n",
    "  \n",
    "data['text'] = data['text'].apply(expand_contractions) #Expand English Contractions (Ex : I've -> I have)\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(remove_accented_chars) #Remove macrons & accented characters\n",
    "\n",
    "#Filtering\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords) \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                               text  \\\n",
      "0               1  theresa may struck deal brussels would give uk...   \n",
      "1               2  agreement may give uk firms continued access e...   \n",
      "2               3  nobody knows brexit secretary dominic raab lik...   \n",
      "3               4  shame asking voters express views second time ...   \n",
      "4               5  dominic raab warns obstacles remain insists en...   \n",
      "...           ...                                                ...   \n",
      "93608       93609  lagence nationale de scurit du mdicament et de...   \n",
      "93609       93610  les dputs ont rcemment adopt un amendement au ...   \n",
      "93610       93611  la mesure doit tre dbattue par les dputs alors...   \n",
      "93611       93612  la taille et la forme du canal pelvignital des...   \n",
      "93612       93613  sant le corps des femmes recle encore de nombr...   \n",
      "\n",
      "       afinn_score afinn_sent_category  \n",
      "0             -1.0            Bad News  \n",
      "1              1.0           Good News  \n",
      "2              0.0             Neutral  \n",
      "3             -5.0            Bad News  \n",
      "4             -4.0            Bad News  \n",
      "...            ...                 ...  \n",
      "93608          0.0             Neutral  \n",
      "93609          1.0           Good News  \n",
      "93610          0.0             Neutral  \n",
      "93611          0.0             Neutral  \n",
      "93612          0.0             Neutral  \n",
      "\n",
      "[93613 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Using Afinn Lexicon to classify the sentence\n",
    "af = Afinn()\n",
    "\n",
    "def afinn_sent_analysis(text):\n",
    "  score = af.score(text)\n",
    "  return score\n",
    "\n",
    "#applying the function to Normalized Comments\n",
    "data['afinn_score'] = [afinn_sent_analysis(comm) for comm in data['text']]\n",
    "\n",
    "#If Afinn Score is more than 0 : The text contains Positive Sentiment\n",
    "                #  less than 0 : The text contains Negative Sentiment\n",
    "                #  equals to 0 : The text contains Neutral Sentiment\n",
    "        \n",
    "def afinn_sent_category(score):\n",
    "  categories = ['Good News','Bad News','Neutral']\n",
    "  if score > 0:\n",
    "    return categories[0]\n",
    "  elif score < 0:\n",
    "    return categories[1]\n",
    "  else:\n",
    "    return categories[2]  \n",
    "\n",
    "data['afinn_sent_category'] = [afinn_sent_category(scr) for scr in data['afinn_score']]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
